# STIndex Data Warehouse

## Overview

The STIndex Data Warehouse is a **hybrid snowflake/star architecture** that enables powerful analytical queries over extracted multi-dimensional data. It combines traditional dimensional modeling with modern capabilities:

- **Dimensional Analytics**: Multi-level hierarchies (Yearâ†’Quarterâ†’Monthâ†’Day, Continentâ†’Countryâ†’Stateâ†’City)
- **Semantic Search**: Vector embeddings with pgvector for similarity search
- **Spatial Queries**: PostGIS-powered geographic queries (radius search, distance calculations)
- **Hybrid Queries**: Combined semantic + dimensional + spatial filtering

## Architecture

```
Unstructured Documents
         â†“
   DimensionalExtractor (LLM-powered)
         â†“
   Structured Entities
         â†“
   ChunkDimensionalLabeler
         â†“
   DimensionalWarehouseETL
         â†“
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   PostgreSQL Warehouse       â”‚
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
   â”‚  â”‚  Dimension Tables      â”‚  â”‚
   â”‚  â”‚  (Temporal, Spatial,   â”‚  â”‚
   â”‚  â”‚   Event, Entity, Doc)  â”‚  â”‚
   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
   â”‚  â”‚  Fact Table            â”‚  â”‚
   â”‚  â”‚  (fact_document_chunks)â”‚  â”‚
   â”‚  â”‚  + Vector Embeddings   â”‚  â”‚
   â”‚  â”‚  + PostGIS Geometries  â”‚  â”‚
   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Installation

### 1. Prerequisites

```bash
# Install PostgreSQL with extensions
sudo apt-get install postgresql-15 postgresql-15-pgvector postgresql-15-postgis

# Or using Docker
docker run -d --name stindex-warehouse \
  -e POSTGRES_PASSWORD=stindex \
  -e POSTGRES_USER=stindex \
  -e POSTGRES_DB=stindex_warehouse \
  -p 5432:5432 \
  pgvector/pgvector:pg15
```

### 2. Create Schema

```bash
# Create database
createdb stindex_warehouse

# Create schema
cd stindex/warehouse/schema
psql stindex_warehouse -f create_schema.sql

# Populate reference data (dates, countries, taxonomies)
psql stindex_warehouse -f populate_dimensions.sql
```

### 3. Configure Connection

Edit `cfg/warehouse.yml`:

```yaml
database:
  connection_string: "postgresql://stindex:stindex@localhost:5432/stindex_warehouse"

features:
  enabled: true
  auto_load: true
```

## Usage

### Basic Usage (Standalone)

```python
from stindex.extraction.dimensional_extraction import DimensionalExtractor
from stindex.warehouse.etl import DimensionalWarehouseETL
from stindex.warehouse.chunk_labeler import DimensionalChunkLabeler

# 1. Extract dimensional information
extractor = DimensionalExtractor(config_path="extract")
results = []

chunks = ["On March 15, 2022, a cyclone hit Broome, Western Australia."]
for chunk in chunks:
    result = extractor.extract(chunk)
    results.append(result)

# 2. Load into warehouse
etl = DimensionalWarehouseETL(
    db_connection_string="postgresql://stindex:stindex@localhost:5432/stindex_warehouse"
)

document_metadata = {
    "title": "Cyclone Report",
    "url": "https://example.com/article",
    "publication_date": "2022-03-16",
    "source": "News Agency",
    "total_chunks": len(chunks),
}

document_text = " ".join(chunks)

chunks_loaded = etl.load_extraction_results(
    extraction_results=results,
    document_metadata=document_metadata,
    document_text=document_text,
)

print(f"âœ“ Loaded {chunks_loaded} chunks into warehouse")
```

### Integrated with Pipeline

```python
from stindex.pipeline.pipeline import STIndexPipeline
from stindex.preprocessing.input_models import InputDocument

# Initialize pipeline with warehouse enabled
pipeline = STIndexPipeline(
    dimension_config="dimensions",
    output_dir="data/output",
    enable_warehouse=True,
    warehouse_config="warehouse"
)

# Process documents
docs = [
    InputDocument.from_url("https://example.com/article1"),
    InputDocument.from_url("https://example.com/article2"),
]

results = pipeline.run_pipeline(docs, load_to_warehouse=True)
# Data automatically loaded into warehouse
```

## Querying the Warehouse

### 1. Dimensional Queries

```sql
-- Event counts by year and quarter
SELECT
    y.year,
    q.quarter,
    COUNT(*) as chunk_count
FROM fact_document_chunks f
JOIN dim_temporal t ON f.temporal_dim_id = t.temporal_id
JOIN dim_date d ON t.date_id = d.date_id
JOIN dim_month m ON d.month_id = m.month_id
JOIN dim_quarter q ON m.quarter_id = q.quarter_id
JOIN dim_year y ON q.year_id = y.year_id
GROUP BY y.year, q.quarter
ORDER BY y.year, q.quarter;
```

### 2. Spatial Queries

```sql
-- Find chunks within 100km of Broome (-18.0, 122.2)
SELECT
    chunk_text,
    ST_Distance(
        location_geom,
        ST_MakePoint(122.2, -18.0)::geography
    ) / 1000.0 AS distance_km
FROM fact_document_chunks
WHERE ST_DWithin(
    location_geom,
    ST_MakePoint(122.2, -18.0)::geography,
    100000  -- 100km in meters
)
ORDER BY distance_km
LIMIT 10;
```

### 3. Hierarchical Rollup

```sql
-- Geographic distribution with rollup
SELECT
    co.country_name,
    st.state_name,
    c.city_name,
    COUNT(*) as chunk_count
FROM fact_document_chunks f
JOIN dim_spatial sp ON f.spatial_dim_id = sp.spatial_id
LEFT JOIN dim_country co ON sp.country_id = co.country_id
LEFT JOIN dim_state st ON sp.state_id = st.state_id
LEFT JOIN dim_city c ON sp.city_id = c.city_id
GROUP BY ROLLUP(co.country_name, st.state_name, c.city_name)
ORDER BY chunk_count DESC;
```

### 4. Label Array Filtering

```sql
-- Filter by hierarchical labels (fast with GIN index)
SELECT chunk_text, spatial_labels, temporal_labels
FROM fact_document_chunks
WHERE
    'Australia' = ANY(spatial_labels)
    AND '2022' = ANY(temporal_labels);
```

### 5. Semantic Search (Future - Phase 4)

```python
from stindex.warehouse.query_api import WarehouseQueryAPI

api = WarehouseQueryAPI(connection_string="...")

# Hybrid search: semantic + dimensional + spatial
results = api.search(
    query_text="tropical cyclones",
    temporal_filter={"year": 2022, "quarter": 1},
    spatial_filter={"country": "Australia", "state": "Western Australia"},
    limit=10
)

for result in results:
    print(f"[{result['similarity']:.3f}] {result['chunk_text']}")
```

## Components

### 1. Chunk Labeler (`chunk_labeler.py`)

Generates hierarchical labels from extraction results:

```python
from stindex.warehouse.chunk_labeler import DimensionalChunkLabeler

labeler = DimensionalChunkLabeler()

labels = labeler.label_chunk(
    chunk_text="On March 15, 2022, a cyclone hit Broome.",
    extraction_result=extraction_result,
    chunk_index=0,
)

print(f"Temporal labels: {labels.temporal_labels}")
# Output: ["2022", "2022-Q1", "2022-03", "2022-03-15"]

print(f"Spatial labels: {labels.spatial_labels}")
# Output: ["Australia", "Western Australia", "Broome"]

print(f"Temporal path: {labels.temporal_path}")
# Output: "2022 > Q1 > March > 2022-03-15"
```

### 2. ETL Pipeline (`etl.py`)

Loads extraction results into warehouse:

- Upserts dimension tables (deduplicates, updates confidence scores)
- Inserts fact records (with vector embeddings and spatial data)
- Transaction management (rollback on failure)
- Caching (reduces database queries by ~80%)

```python
from stindex.warehouse.etl import DimensionalWarehouseETL

etl = DimensionalWarehouseETL(
    db_connection_string="postgresql://...",
    batch_size=100,
)

chunks_loaded = etl.load_extraction_results(
    extraction_results=[result1, result2, ...],
    document_metadata=metadata,
    document_text=full_text,
)
```

### 3. Query API (`query_api.py` - Phase 4)

High-level API for warehouse queries:

- Semantic search (vector similarity)
- Dimensional filtering (temporal, spatial, event)
- Aggregation (rollup, drill-down)
- Spatial queries (radius, bounding box)
- Hybrid queries (combining all above)

## Schema Summary

### Dimension Tables

| Dimension | Tables | Hierarchy Levels |
|-----------|--------|------------------|
| Temporal | dim_year, dim_quarter, dim_month, dim_date, dim_time, dim_temporal | 5+ levels |
| Spatial | dim_continent, dim_country, dim_state, dim_region, dim_city, dim_suburb, dim_address, dim_spatial | 7+ levels |
| Event | dim_event_category, dim_event_type, dim_event_subtype, dim_event | 3 levels |
| Entity | dim_entity_category, dim_entity_type, dim_entity | 2 levels |
| Document | dim_document | Flat (no hierarchy) |

### Fact Table

`fact_document_chunks` - Central table with:

- **Dimensional FKs**: Links to all dimension tables
- **Vector Embeddings**: 1536-d vectors for semantic search (pgvector)
- **Spatial Data**: PostGIS GEOGRAPHY points
- **Label Arrays**: Hierarchical labels for fast filtering (GIN indexes)
- **Hierarchy Paths**: Human-readable paths for drill-down
- **Confidence Scores**: Overall and per-dimension confidence
- **Metadata**: LLM provider, model, token usage, extraction config

## Performance

### Indexes

- **Vector Index (IVFFlat)**: ~10ms for 1M vectors (100 lists)
- **Spatial Index (GIST)**: ~5ms for radius queries
- **GIN Indexes**: ~1ms for label array filtering
- **B-tree Indexes**: Standard dimensional FK lookups

### Caching

ETL pipeline includes dimension caching:

- Temporal cache: 10,000 entries
- Spatial cache: 10,000 entries
- Document cache: 1,000 entries

Reduces database queries by ~80% for repeated extractions.

### Scaling

- **< 1M chunks**: Use IVFFlat vector index
- **> 1M chunks**: Switch to HNSW vector index
- **> 10M chunks**: Enable partitioning by extraction timestamp
- **> 100M chunks**: Consider distributed setup (Citus, TimescaleDB)

## Configuration

See `cfg/warehouse.yml` for all configuration options:

- Database connection settings
- ETL pipeline settings (batch size, caching)
- Vector embedding settings
- Performance tuning
- Logging and monitoring

## Development Status

### âœ… Completed (Phases 1-3.5)

- [x] Schema design (snowflake/star hybrid)
- [x] Dimension tables (temporal, spatial, event, entity, document)
- [x] Fact table with hybrid capabilities
- [x] SQL scripts (schema creation, dimension population)
- [x] Chunk labeling (hierarchical label generation)
- [x] ETL pipeline (dimension upserts, fact inserts, caching)
- [x] Pipeline integration (STIndexPipeline with enable_warehouse flag)
- [x] Configuration file
- [x] Documentation

### ðŸš§ In Progress (Phases 4-5)

- [ ] Query API (hybrid search, aggregations, spatial queries)
- [ ] Unit tests
- [ ] Integration tests
- [ ] Performance optimization

### ðŸ“‹ Planned (Future)

- [ ] Real-time streaming ETL
- [ ] Query caching and materialization
- [ ] BI tool connectors (Tableau, Power BI)
- [ ] API server (REST/GraphQL)
- [ ] Monitoring dashboard
- [ ] Auto-scaling and sharding

## Troubleshooting

### Connection Errors

```bash
# Check PostgreSQL is running
sudo systemctl status postgresql

# Check extensions are installed
psql stindex_warehouse -c "SELECT * FROM pg_extension;"
```

### Schema Issues

```bash
# Drop and recreate schema
psql stindex_warehouse -c "DROP SCHEMA public CASCADE; CREATE SCHEMA public;"
psql stindex_warehouse -f stindex/warehouse/schema/create_schema.sql
psql stindex_warehouse -f stindex/warehouse/schema/populate_dimensions.sql
```

### Performance Issues

```sql
-- Check index usage
SELECT schemaname, tablename, indexname, idx_scan
FROM pg_stat_user_indexes
ORDER BY idx_scan ASC
LIMIT 10;

-- Rebuild indexes
REINDEX TABLE fact_document_chunks;

-- Analyze tables
ANALYZE fact_document_chunks;
```

## References

- [Schema Documentation](schema/README.md) - Detailed schema design
- [dimensional-data-warehouse.md](../../docs/research/dimensional-data-warehouse.md) - Research and design decisions
- [PostgreSQL Documentation](https://www.postgresql.org/docs/)
- [pgvector Documentation](https://github.com/pgvector/pgvector)
- [PostGIS Documentation](https://postgis.net/documentation/)

## License

See project root LICENSE file.
