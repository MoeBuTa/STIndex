# vLLM Configuration
# Configuration for vLLM-based inference with OpenAI-compatible API

llm:
  # Provider type
  llm_provider: vllm

  # Model name/identifier (HuggingFace model ID or local path)
  # This is the DEFAULT model if not specified via --model argument
  # Can be overridden at runtime: stindex extract "text" --model "Qwen/Qwen3-8B"
  model_name: Qwen/Qwen3-8B

  # Generation parameters
  temperature: 0.0
  max_tokens: 4096

  # Client configuration - connects to router
  router_url: "http://localhost:8000"  # Router endpoint (not direct vLLM server)

  # Auto-start configuration
  # If true, automatically start vLLM servers if not running (default: true)
  # Use --auto-start flag to disable for a specific run
  auto_start: true

  # Auto-stop configuration (timeout-based)
  # Automatically stop servers after N seconds of inactivity
  # Set to 0 or null to disable auto-stop (servers run indefinitely)
  # Recommended: 300 (5 minutes), 600 (10 minutes), or 1800 (30 minutes)
  auto_stop_timeout: 600  # Stop after 10 minutes of inactivity

  # Disable custom all-reduce (for compatibility)
  # vLLM's custom all-reduce is faster (~5-10%) but only works on specific GPU topologies
  # If you get "custom_all_reduce.cuh invalid argument" errors, set this to true
  # true = Use standard NCCL (more compatible, slightly slower)
  # false = Use vLLM custom kernel (faster, may fail on some systems)
  disable_custom_all_reduce: true

# ============================================================================
# Router Configuration
# ============================================================================
# Multi-model router that routes requests to different vLLM backend servers
# based on model name specified in requests
router:
  # Router server settings
  host: "0.0.0.0"
  port: 8000

  # Model registry - maps model names to backend vLLM server URLs
  # Format: "model_name": "backend_url"
  models:
#    "Qwen/Qwen3-4B-Instruct-2507": "http://localhost:8001"
    "Qwen/Qwen3-8B": "http://localhost:8002"

# ============================================================================
# vLLM Backend Server Configuration
# ============================================================================
# Used by scripts/start_vllm_servers.sh to start backend vLLM servers
# Each backend serves one model on a specific port
server:
  # Models to load (list of model configurations)
  # Each model gets its own server instance
  models:
    # Qwen3-4B-Instruct for general extraction
    - name: "Qwen/Qwen3-4B-Instruct-2507"
      port: 8001
      enabled: true
      gpu_memory_utilization: 0.7  # Use 90% of GPU memory
      trust_remote_code: true
      tensor_parallel_size: 1  # Optional: only needed if multiple models enabled. If only one model enabled, auto-uses all GPUs
    - name: "Qwen/Qwen3-8B"
      port: 8002
      enabled: true
      gpu_memory_utilization: 0.7
      trust_remote_code: true
      tensor_parallel_size: 1  # Use 2 GPUs for this model

    - name: "Qwen/Qwen3-0.6B"
      port: 8003
      enabled: true
      gpu_memory_utilization: 0.7
      trust_remote_code: true
      tensor_parallel_size: 1  # Use 2 GPUs for this model
