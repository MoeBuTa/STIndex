# MS-SWIFT Configuration
# Configuration for MS-SWIFT framework with vLLM backend

llm:
  # Provider type
  llm_provider: ms_swift

  # Model configuration
  model_name: Qwen3-8B  # Model name as reported by MS-SWIFT server (not HuggingFace ID)

  # Generation parameters
  temperature: 0.0  # Deterministic output
  max_tokens: 4096  # Maximum tokens to generate

  # Advanced sampling parameters
  seed: null  # Random seed for reproducibility (null = random)
  top_p: 1.0  # Top-p (nucleus) sampling
  top_k: -1  # Top-k sampling (-1 = disabled)
  stream: false  # Enable streaming responses

  # Server configuration
  base_url: "http://localhost:8001"  # MS-SWIFT deployment endpoint (updated to actual port)

  # Client type selection
  # Options: 'auto' (prefer MS-SWIFT native), 'swift' (force native), 'openai' (force OpenAI SDK)
  client_type: auto

# MS-SWIFT Deployment Configuration
deployment:
  # Model to deploy
  model: Qwen/Qwen3-8B

  # Inference backend (vLLM for efficient serving)
  infer_backend: vllm

  # Use HuggingFace downloaded models
  use_hf: true

  # Server settings
  host: "0.0.0.0"
  port: 8000

  # Logging settings
  verbose: true  # Enable verbose logging
  log_interval: 20  # Log interval for batches

  # Optional: Fine-tuned model checkpoint
  ckpt_dir: null  # Path to checkpoint directory (null = use base model)
  merge_lora: false  # Merge LoRA weights before deployment

  # Optional: Multi-LoRA deployment
  lora_modules: null  # Dict of {name: path} for multiple LoRA adapters
  # Example:
  # lora_modules:
  #   adapter1: /path/to/adapter1
  #   adapter2: /path/to/adapter2

  # vLLM backend settings
  vllm:
    tensor_parallel_size: 1  # Number of GPUs for tensor parallelism
    gpu_memory_utilization: 0.2  # GPU memory utilization (reduced to fit available memory)
    max_model_len: 8192  # Maximum context length (reduced from 40960 to fit in available KV cache)
    trust_remote_code: true  # Trust remote code for model loading
    dtype: auto  # Model dtype (auto, float16, bfloat16, float32)
    enable_lora: false  # Enable LoRA support in vLLM

# Training Configuration (for DeepSpeed)
training:
  # Training framework
  framework: deepspeed

  # DeepSpeed configuration
  deepspeed:
    stage: 2  # ZeRO stage (0, 1, 2, 3)
    offload_optimizer: false  # Offload optimizer to CPU
    offload_param: false  # Offload parameters to CPU

  # Training hyperparameters
  learning_rate: 2.0e-5
  batch_size: 1
  gradient_accumulation_steps: 16
  num_train_epochs: 3
  warmup_ratio: 0.03
  weight_decay: 0.01

  # LoRA configuration (optional)
  lora:
    enabled: false
    r: 8
    lora_alpha: 32
    lora_dropout: 0.1
    target_modules: ["q_proj", "k_proj", "v_proj"]
