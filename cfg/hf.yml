# HuggingFace LLM Configuration
# Automatically supports both single-GPU and multi-GPU modes
# The model will be loaded into GPU memory when the HF server starts

llm:
  # Provider type
  llm_provider: hf

  # Model name/identifier (HuggingFace model ID or local path)
  model_name: Qwen/Qwen3-4B-Thinking-2507

  # Optional: Local model path (if model is stored locally)
  model_path: ""

  # Device: auto (uses GPU if available), cuda, cpu
  device: auto

  # Data type for model weights (float16 recommended for GPU)
  torch_dtype: float16

  # Generation parameters
  temperature: 0.0
  max_tokens: 2048

  # Server Configuration
  # ------------------
  # Dynamic server detection based on base port and number of GPUs
  # Servers will be auto-detected at: base_port, base_port+1, base_port+2, ...

  base_port: 8001  # Starting port for server detection
  num_gpus: auto   # Auto-detect available servers, or specify number (1, 2, 4, etc.)

  # Load balancing strategy: "round_robin" or "random"
  load_balancing: round_robin

# How to use:
# -----------
# Multi-GPU mode (auto-detect):
#   1. Start servers: ./scripts/start_servers.sh  (auto-detects GPUs)
#   2. Servers run on ports 8001, 8002, 8003, 8004... (one per GPU)
#   3. Client automatically detects and uses all available servers
#
# Single-GPU mode:
#   1. Start server: STINDEX_NUM_GPUS=1 ./scripts/start_servers.sh
#   2. Set num_gpus: 1 in this config (or leave as auto)
#
# Custom port range:
#   1. Start with custom base port: STINDEX_HF_BASE_PORT=9001 ./scripts/start_servers.sh
#   2. Update base_port in this config to match

