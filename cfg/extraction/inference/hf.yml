# HuggingFace (MS-SWIFT) Configuration
# Use with: stindex extract --config hf [--model ...] [--base-url ...]
# Client model/temperature/max_tokens/base_url come from PROVIDER_DEFAULTS
# in stindex/utils/config.py and can be overridden via CLI flags.

llm:
  llm_provider: hf

deployment:
  # Model to deploy (HuggingFace model ID or local path)
  model: Qwen/Qwen3-4B-Instruct-2507

  # Inference backend (vllm recommended for efficiency)
  infer_backend: vllm

  # Use HuggingFace Hub downloaded models (true = use cached models from ~/.cache/huggingface)
  use_hf: true

  # Server network settings
  host: "0.0.0.0"  # Listen on all interfaces
  port: 8001        # Server port â€” must match PROVIDER_DEFAULTS["hf"]["base_url"] port

  # Logging settings
  verbose: true      # Enable detailed logging
  log_interval: 20   # Log every N batches

  # Result path for inference logs (null = disable result logging)
  result_path: null

  # Optional: Fine-tuned model checkpoint
  ckpt_dir: null     # Path to checkpoint directory (null = use base model)
  merge_lora: false  # Merge LoRA weights before deployment

  # Optional: Multi-LoRA deployment (serve multiple adapters)
  lora_modules: null

  # vLLM backend settings
  vllm:
    tensor_parallel_size: 1       # 4B model fits on 1 GPU
    gpu_memory_utilization: 0.85  # Fraction of GPU memory to use (0.0-1.0)
    max_model_len: 8192           # Maximum sequence length (tokens)
    dtype: auto                   # Data type: auto, float16, bfloat16, float32
    trust_remote_code: true       # Allow custom model code execution
